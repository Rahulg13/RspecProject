---
title: "Week2 Assignment"
author: "Rahulg13"
date: "22/07/2020"
output: html_document
---

## Data acquisition 

Data was downloaded from the provided link. The folder was named "final" and stored within the R project directory. 

#### Link for the download was as follows - 
<https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip>

The files contained in the folder can be seen as follows - 

```{r}
list.files("final/")
list.files("final/en_us/")
```

As can be seen above, there are three files in en_US (i.e. US English) containing data for blogs, news and twitter. 

## Data loading in R
Using the file paths seen above, the data was loaded in R using following code - 

```{r, results = 'hide', cache= TRUE}
con1 <- file("final/en_US/en_US.blogs.txt")
con2 <- file("final/en_US/en_US.news.txt")
con3 <- file("final/en_US/en_US.twitter.txt")

blogs <- readLines(con1)
news <- readLines(con2)
twitter <- readLines(con3)
```



## Exploratory Data Analysis and Summaries 

### Calculating Basic Statistics of the files - mean, maximum and minimum number of characters in lines within a file. It also calculates the total number of lines in dataset. 

#### Blogs 
```{r, results= 'hide'}
mean(nchar(blogs))
max(nchar(blogs))
min(nchar(blogs))
```

```{r, echo=FALSE}
mean(nchar(blogs))
max(nchar(blogs))
min(nchar(blogs))
```

Similarly, results for News and twitter were found out. 
#### News 
```{r, echo=FALSE}
mean(nchar(news))
max(nchar(news))
min(nchar(news))
```

#### Twitter 
Notably, This must have max number of character as 140 obviously, so it checks whether our calculations are right or not. 
```{r, echo=FALSE}
mean(nchar(twitter))
max(nchar(twitter))
min(nchar(twitter))
```

#### Size of the three sets is as follows (size = number of lines) - 
```{r, echo=FALSE}
print("Number of lines in Blogs")
length(blogs)
print("Number of lines in News")
length(news)
print("Number of lines in Twitter")
length(twitter)
```


## Basic Plots 

### Preparing data for plotting
```{r}
object.size(blogs)
```

The figure of memory used is very high and thus, loading all the data into memory might not be efficient. So, it is decided to load a random 10% sample of all the data and do the further analysis based on that. "rbinom" function was used to create the random sample and then, the samples were written in separate files.

```{r, results='hide', cache= TRUE}
set.seed(1313)

l1 <- length(blogs) 

select1 <- rbinom(l1, 1, 0.1)
select1 <- (select1 == 1)

file.create("en_US/SampleENBlogs.txt")
con1a <- file("en_US/SampleENBlogs.txt")

writeLines(blogs[select1], con1a)
```

```{r, echo=FALSE, results='hide'}
l2 <- length(news) 
l3 <- length(twitter) 


select2 <- rbinom(l2, 1, 0.1)
select2 <- (select2 == 1)

select3 <- rbinom(l3, 1, 0.1)
select3 <- (select3 == 1)

file.create("en_US/SampleENNews.txt")
con2a <- file("en_US/SampleENNews.txt")
file.create("en_US/SampleENTwitter.txt")
con3a <- file("en_US/SampleENTwitter.txt")

writeLines(news[select2], con2a)
writeLines(twitter[select3], con3a)
```

```{r}
list.files("en_US/")

```


*Thus, 3 files containing sampled data are created and further exploration is done using them. *

#### Creation of Corpus 
For creation of Corpus, VCorpus is used because it is volatile allows dynamic processing and thus, processing of DocumentTermMatrix to a smaller, more manageable size which is the target of our subsequent operations. 

For Cleaning the data, following transformations have been used - 
1. Transformation to Lower case 
2. Removing punctuation marks 
3. Removing stopwords from the data 
4. Stripping Whitespace 

```{r, echo= TRUE, results = "hide", cache=TRUE}
library(tm)
con1 <- file("en_US/SampleENBlogs.txt")
con2 <- file("en_US/SampleENNews.txt")
con3 <- file("en_US/SampleENTwitter.txt")

blogs <- readLines(con1)
news <- readLines(con2)
twitter <- readLines(con3)

corpus1 <- VCorpus(VectorSource(blogs))
corpus1 <- tm_map(corpus1, content_transformer(tolower))
corpus1 <- tm_map(corpus1, FUN = removePunctuation)
corpus1 <- tm_map(corpus1, FUN = removeWords, stopwords(kind = "en"))
corpus1 <- tm_map(corpus1, FUN = stripWhitespace)
```

#### Calculating Document Term Matrix from the corpora created
Below is a sample code for Blogs, similar codes are written for news as well as twitter.
```{r, results = 'hide', cache=TRUE}
dtm11 <- DocumentTermMatrix(corpus1)
dtm11 <- removeSparseTerms(dtm11, 0.995)
mat11 <- as.matrix(dtm11)
wordfreq11 <- colSums(mat11)
wordfreq11 <- sort(wordfreq11, decreasing = TRUE)
```

*Removing sparse terms is necessary, otherwise matrix is too large and memory gets exhausted. However, too low a sparse threshold will give only few values, which renders the calculations redundant.*

For bi-gram tokenizer, the code is as follows 
```{r,results='hide', cache=TRUE}
library(RWeka)
tokenarg <- function(x) { NGramTokenizer(x,Weka_control(min = 2, max = 2)) } 
dtm12 <- DocumentTermMatrix(corpus1, control = list(tokenize = tokenarg))
dtm12 <- removeSparseTerms(dtm12, 0.9995)
mat12 <- as.matrix(dtm12)
wordfreq12 <- colSums(mat12)
wordfreq12 <- sort(wordfreq12, decreasing = TRUE)

```

Similar code was made for tri-gram tokenizer too. 

```{r, results='hide', echo=FALSE, cache=TRUE}
# tri-gram corpus1 
tokenarg <- function(x) { NGramTokenizer(x,Weka_control(min = 3, max = 3)) } 
dtm13 <- DocumentTermMatrix(corpus1, control = list(tokenize = tokenarg))
dtm13 <- removeSparseTerms(dtm13, 0.9999)
mat13 <- as.matrix(dtm13)
wordfreq13 <- colSums(mat13)
wordfreq13 <- sort(wordfreq13, decreasing = TRUE)

```


### vacate the space 
```{r}
object.size(corpus1)
object.size(mat11)
object.size(mat12)
object.size(mat13)
rm(corpus1)
rm(mat11)
rm(mat12)
rm(mat13)
```


## TIME TO PLOT

```{r, echo=FALSE, cache=TRUE}
library(ggplot2)
library(dplyr)


wordfreq11 <- as.data.frame(wordfreq11)
gram1 <- rownames(wordfreq11)
wordfreq11 <- wordfreq11 %>% mutate(gram1 = gram1)
wordfreq11$gram1 <- factor(wordfreq11$gram1, levels = wordfreq11$gram1)

p1 <- ggplot(data = wordfreq11[1:10, ] ,aes(x = gram1, y = wordfreq11, fill = gram1))
p1 <- p1 + geom_bar(stat = "identity")
p1 <- p1 + theme(axis.text.x = element_blank())
p1 <- p1 + geom_text(label = wordfreq11[1:10,1], vjust = 0)

wordfreq12 <- as.data.frame(wordfreq12)
gram1 <- rownames(wordfreq12)
wordfreq12 <- wordfreq12 %>% mutate(gram1 = gram1)
wordfreq12$gram1 <- factor(wordfreq12$gram1, levels = wordfreq12$gram1)

p2 <- ggplot(data = wordfreq12[1:10, ] ,aes(x = gram1, y = wordfreq12, fill = gram1))
p2 <- p2 + geom_bar(stat = "identity")
p2 <- p2 + theme(axis.text.x = element_blank())
p2 <- p2 + geom_text(label = wordfreq12[1:10,1], vjust = 0)

wordfreq13 <- as.data.frame(wordfreq13)
gram1 <- rownames(wordfreq13)
wordfreq13 <- wordfreq13 %>% mutate(gram1 = gram1)
wordfreq13$gram1 <- factor(wordfreq13$gram1, levels = wordfreq13$gram1)

p3 <- ggplot(data = wordfreq13[1:10, ] ,aes(x = gram1, y = wordfreq13, fill = gram1))
p3 <- p3 + geom_bar(stat = "identity")
p3 <- p3 + theme(axis.text.x = element_blank())
p3 <- p3 + geom_text(label = wordfreq13[1:10,1], vjust = 0)


p1
p2
p3
```



## Plan for creating Shiny Algorithm 
Thus, we get a good sense of the data we are dealing with. Some interesting findings are as follows - 

1. In 3-gram model, we get only 248 3-grams with sparsity of 99.99%, otherwise the memory gets loaded. In 2-gram model the number of 2-grams is 682 which is better. 

2. In the top 10 tri-grams, new york appears 3 times which also shows dataset is perhaps good only for American applications and may not be useful for other regions. 

3. Size of the corpora and matrices (from dtm) are huge, more than 350 MB in each case. Thus, after calculating the word frequency data frame, they should be deleted to save the memory and application fast. Notably, only word frequency data frames are useful in final prediction algorithms. 

With these takeaways, the plan for a Shiny app moves forward. 